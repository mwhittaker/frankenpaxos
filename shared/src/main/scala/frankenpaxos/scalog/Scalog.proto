// # Scalog
//
// This directory contains a simplified, not very live implementation of Scalog
// [1]. This implemention is intended to be used as a baseline against which we
// compare Compartmentalized Paxos, so we don't implement the full protocol.
// Notably, we make the following simplifications.
//
//   1. We assume that servers (a.k.a. batchers) don't fail. If they do, the
//      protocol grinds to a halt. We don't detect their failure or handle
//      their failure.
//   2. We don't implement a tree of aggregators. Instead, we have a single
//      aggregator node. As with the servers, we assume the aggregator does not
//      fail, we do not monitor its failure, and we do not handle its failure.
//      If it fails, the entire protocol grinds to a halt.
//
// Here are a couple high level design decisions that are not obvious from the
// paper.
//
//   1. Servers push their updates to the aggregator. The aggregator does not
//      poll updates from the servers.
//   2. Clients send commands to randomly chosen servers.
//   3. The aggregator proposes cuts to Paxos, but these cuts may arrive out of
//      order. This leads to a sequence of cuts that may not be monotonically
//      increasing. Thus, the aggregator filters out non-monotonically
//      increasing cuts. For example, consider a deployment with two servers in
//      a single shard. Paxos might choose cut [1, 1] and then [0, 0]. The
//      aggregator would filter out [0, 0]. We call the unfiltered cuts _raw
//      cuts_, and we call filtered cuts just _cuts_.
//
// Here is a protocol cheatsheet, similar to [2].
//
// ## Normal Case Processing
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        |        | -----> |        |        Phase1a
//      |        |        |        | <----- |        |        Phase1b
//      | -----> |        |        |        |        |        ClientRequest
//      |        | -----> |        |        |        |        ShardInfo
//      |        |        | -----> |        |        |        ProposeCut
//      |        |        |        | -----> |        |        Phase2a
//      |        |        |        | <----- |        |        Phase2b
//      |        |        | <----- |        |        |        RawCutChosen
//      |        |        |        | ----.  |        |        RawCutChosen
//      |        |        |        | <---'  |        |
//      |        | <----- |        |        |        |        CutChosen
//      |        | -------^--------^--------^------> |        Chosen
//      | <------^--------^--------^--------^------- |        ClientReply
//
// ## Learning Who The Leader Is
//
// If the leader changes, how does the aggregator know who to send messages to?
// If the aggregator hasn't heard back from the leader in a while, it
// broadcasts a LeaderInfoRequest message to all the proposers. The leader
// responds to the aggregator informing the aggregator of its leadership.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        | -----> |        |        |        LeaderInfoRequest
//      |        |        | <----- |        |        |        LeaderInfoReply
//
// ## Recovering Holes
//
// If a replica has a hole in its log for too long, it sends a Recover message
// to the aggregator. The aggregator finds the corresponding cut and sends it
// again to the server. The server forwards chosen commands for duplicated
// CutChosen messages, so it will inform the replicas again.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        | <------^--------^------- |        Recover
//      |        | <----- |        |        |        |        CutChosen
//      |        | -------^--------^--------^------> |        Chosen
//
// ## Nacks
//
// Acceptors nack messages in stale rounds.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        |        | <----- |        |        Nack
//
// [1]: https://www.usenix.org/system/files/nsdi20-paper-ding.pdf
// [2]: https://ndpsoftware.com/git-cheatsheet.html

syntax = "proto2";

package frankenpaxos.multipaxos;

import "scalapb/scalapb.proto";

option (scalapb.options) = {
  package_name: "frankenpaxos.multipaxos"
  flat_package: true
};

// Helper messages. ////////////////////////////////////////////////////////////
message Noop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message CommandId {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // A client's address, pseudonym, and id uniquely identify a command.
  required bytes client_address = 1;
  required int32 client_pseudonym = 2;
  required int32 client_id = 3;
}

message Command {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes command = 2;
}

message CommandBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated Command command = 1;
}

message CommandBatchOrNoop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof value {
    CommandBatch command_batch = 1;
    Noop noop = 2;
  }
}

// Protocol messages. //////////////////////////////////////////////////////////
message ClientRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required Command command = 1;
}

message ClientRequestBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandBatch batch = 1;
}

message Phase1a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;

  // The leader knows that all entries in slots less than `chosenWatermark`
  // have been chosen. Acceptors do not have to include slots below
  // `chosenWatermark` in their phase1b response.
  //
  // The leader may know that some entries larger than `chosenWatermark` have
  // also been chosen, but that's okay. It's not unsafe for acceptors to return
  // too much information.
  required int32 chosen_watermark = 2;
}

message Phase1bSlotInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 vote_round = 2;
  required CommandBatchOrNoop vote_value = 3;
}

message Phase1b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 group_index = 1;
  required int32 acceptor_index = 2;
  required int32 round = 3;
  repeated Phase1bSlotInfo info = 4;
}

message Phase2a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 round = 2;
  required CommandBatchOrNoop command_batch_or_noop = 3;
}

message Phase2b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 group_index = 1;
  required int32 acceptor_index = 2;
  required int32 slot = 3;
  required int32 round = 4;
}

message Chosen {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required CommandBatchOrNoop command_batch_or_noop = 2;
}

message ClientReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required int32 slot = 2;
  required bytes result = 3;
}

message ClientReplyBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated ClientReply batch = 1;
}

message MaxSlotRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
}

message MaxSlotReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required int32 group_index = 2;
  required int32 acceptor_index = 3;
  required int32 slot = 4;
}

message BatchMaxSlotRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 read_batcher_index = 1;
  required int32 read_batcher_id = 2;
}

message BatchMaxSlotReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 read_batcher_index = 1;
  required int32 read_batcher_id = 2;
  required int32 acceptor_index = 3;
  required int32 slot = 4;
}

message ReadRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // If a client sends a ReadRequest to a ReadBatcher, the slot is set to -1.
  required int32 slot = 1;
  required Command command = 2;
}

message ReadRequestBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  repeated Command command = 2;
}

message SequentialReadRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required Command command = 2;
}

message SequentialReadRequestBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  repeated Command command = 2;
}

message EventualReadRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required Command command = 1;
}

message EventualReadRequestBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated Command command = 1;
}

message ReadReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required int32 slot = 2;
  required bytes result = 3;
}

message ReadReplyBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated ReadReply batch = 1;
}

// If a client or batcher sends a request to a leader, but the leader is
// inactive, then the leader sends back a NotLeader{Client,Batcher} message.
// The client or batcher then sends a LeaderInfoRequest{Client,Batcher} request
// to all leaders, and the active leader replies with a
// LeaderInfoReply{Client,Batcher} request with its current round.
message NotLeaderClient {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoRequestClient {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoReplyClient {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message NotLeaderBatcher {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required ClientRequestBatch client_request_batch = 1;
}

message LeaderInfoRequestBatcher {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoReplyBatcher {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message Nack {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message ChosenWatermark {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // Replicas periodically send ChosenWatermark messages to the leaders
  // informing them that every log entry smaller than `slot` has been chosen.
  // For example, if `slot` is 3, then slots 0, 1, and 2 have been chosen.
  // Slots above `slot` may also be chosen, but that's okay.
  //
  // If replicas didn't send these messages, then leaders would have no idea
  // which commands have been chosen and which haven't. This can significantly
  // slow things down after a leader change.
  required int32 slot = 1;
}

message Recover {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // Replicas execute logs in prefix order. Thus, if the log permanently has a
  // hole in it, the algorithm remains forever blocked. To solve this, if a
  // replica notices a hole in its log for a certain amount of time, it sends a
  // Recover message to the leader to get the hole plugged.
  required int32 slot = 1;
}

// Inbound messages. ///////////////////////////////////////////////////////////
message ClientInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientReply client_reply = 1;
    NotLeaderClient not_leader_client = 2;
    LeaderInfoReplyClient leader_info_reply_client = 3;
    MaxSlotReply max_slot_reply = 4;
    ReadReply read_reply = 5;
  }
}

message BatcherInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientRequest client_request = 1;
    NotLeaderBatcher not_leader_batcher = 2;
    LeaderInfoReplyBatcher leader_info_reply_batcher = 3;
  }
}

message ReadBatcherInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ReadRequest read_request = 1;
    SequentialReadRequest sequential_read_request = 2;
    EventualReadRequest eventual_read_request = 3;
    BatchMaxSlotReply batch_max_slot_reply = 4;
  }
}

message LeaderInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase1b phase1b = 1;
    ClientRequest client_request = 2;
    ClientRequestBatch client_request_batch = 3;
    LeaderInfoRequestClient leader_info_request_client = 4;
    LeaderInfoRequestBatcher leader_info_request_batcher = 5;
    Nack nack = 6;
    ChosenWatermark chosen_watermark = 7;
    Recover recover = 8;
  }
}

message ProxyLeaderInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase2a phase2a = 1;
    Phase2b phase2b = 2;
  }
}

message AcceptorInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase1a phase1a = 1;
    Phase2a phase2a = 2;
    MaxSlotRequest max_slot_request = 3;
    BatchMaxSlotRequest batch_max_slot_request = 4;
  }
}

message ReplicaInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Chosen chosen = 1;
    ReadRequest read_request = 2;
    SequentialReadRequest sequential_read_request = 3;
    EventualReadRequest eventual_read_request = 4;
    ReadRequestBatch read_request_batch = 5;
    SequentialReadRequestBatch sequential_read_request_batch = 6;
    EventualReadRequestBatch eventual_read_request_batch = 7;
  }
}

message ProxyReplicaInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientReplyBatch client_reply_batch = 1;
    ReadReplyBatch read_reply_batch = 2;
    ChosenWatermark chosen_watermark = 3;
    Recover recover = 4;
  }
}
