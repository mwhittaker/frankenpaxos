// # Scalog
//
// This directory contains a simplified, not very live implementation of Scalog
// [1]. This implemention is intended to be used as a baseline against which we
// compare Compartmentalized Paxos, so we don't implement the full protocol.
// Notably, we make the following simplifications.
//
//   1. We assume that servers (a.k.a. batchers) don't fail. If they do, the
//      protocol grinds to a halt. We don't detect their failure or handle
//      their failure.
//   2. We don't implement a tree of aggregators. Instead, we have a single
//      aggregator node. As with the servers, we assume the aggregator does not
//      fail, we do not monitor its failure, and we do not handle its failure.
//      If it fails, the entire protocol grinds to a halt.
//   3. Servers are never added or removed. We have a fixed set of servers.
//   4. We assume every shard has the same number of servers.
//
// Here are a couple high level design decisions that are not obvious from the
// paper.
//
//   1. Servers push their updates to the aggregator. The aggregator does not
//      poll updates from the servers.
//   2. Clients send commands to randomly chosen servers.
//   3. The aggregator proposes cuts to Paxos, but these cuts may arrive out of
//      order. This leads to a sequence of cuts that may not be monotonically
//      increasing. Thus, the aggregator filters out non-monotonically
//      increasing cuts. For example, consider a deployment with two servers in
//      a single shard. Paxos might choose cut [1, 1] and then [0, 0]. The
//      aggregator would filter out [0, 0]. We call the unfiltered cuts _raw
//      cuts_, and we call filtered cuts just _cuts_.
//
// Here is a protocol cheatsheet, similar to [2].
//
// ## Normal Case Processing
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        |        | -----> |        |        Phase1a
//      |        |        |        | <----- |        |        Phase1b
//      | -----> |        |        |        |        |        ClientRequest
//      |        | ---.   |        |        |        |        Backup
//      |        | <--'   |        |        |        |
//      |        | -----> |        |        |        |        ShardInfo
//      |        |        | -----> |        |        |        ProposeCut
//      |        |        |        | -----> |        |        Phase2a
//      |        |        |        | <----- |        |        Phase2b
//      |        |        | <----- |        |        |        RawCutChosen
//      |        |        |        | ----.  |        |        RawCutChosen
//      |        |        |        | <---'  |        |
//      |        | <----- |        |        |        |        CutChosen
//      |        | -------^--------^--------^------> |        Chosen
//      | <------^--------^--------^--------^------- |        ClientReply
//
// ## Learning Who The Leader Is
//
// If the leader changes, how does the aggregator know who to send messages to?
// If the aggregator hasn't heard back from the leader in a while, it
// broadcasts a LeaderInfoRequest message to all the proposers. The leader
// responds to the aggregator informing the aggregator of its leadership.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        | -----> |        |        |        LeaderInfoRequest
//      |        |        | <----- |        |        |        LeaderInfoReply
//
// ## Recovering Holes
//
// If a replica has a hole in its log for too long, it sends a Recover message
// to the aggregator. The aggregator finds the corresponding cut and sends it
// again to the server. The server forwards chosen commands for duplicated
// CutChosen messages, so it will inform the replicas again.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        | <------^--------^------- |        Recover
//      |        | <----- |        |        |        |        CutChosen
//      |        | -------^--------^--------^------> |        Chosen
//
// The aggregator may also have holes in its log, in which case it sends a
// Recover to the Paxos leader. If the leader doesn't have the slot, it ignores
// the request.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        | -----> |        |        |        Recover
//      |        |        | <----- |        |        |        RawCutChosen
//
// A server may also have a hole in one of its backup logs.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        | ---.   |        |        |        |        Recover
//      |        | <--'   |        |        |        |
//
// ## Nacks
//
// Acceptors nack messages in stale rounds.
//
//   Client   Server Aggregator Leader  Acceptor  Replica
//      |        |        |        | <----- |        |        Nack
//
// [1]: https://www.usenix.org/system/files/nsdi20-paper-ding.pdf
// [2]: https://ndpsoftware.com/git-cheatsheet.html

syntax = "proto2";

package frankenpaxos.scalog;

import "scalapb/scalapb.proto";

option (scalapb.options) = {
  package_name: "frankenpaxos.scalog"
  flat_package: true
};

// Helper messages. ////////////////////////////////////////////////////////////
message Noop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message CommandId {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // A client's address, pseudonym, and id uniquely identify a command.
  required bytes client_address = 1;
  required int32 client_pseudonym = 2;
  required int32 client_id = 3;
}

message Command {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required bytes command = 2;
}

message CommandBatch {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  repeated Command command = 1;
}

message CommandBatchOrNoop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof value {
    CommandBatch command_batch = 1;
    Noop noop = 2;
  }
}

message GlobalCut {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // A global cut is the concatenation of shard cuts from every shard. For
  // example, consider a Scalog deployment with 2 shards with cuts [2, 2] and
  // [1, 3]. The corresponding global cut is [2, 2, 1, 3]. See ShardInfo for
  // more information on cuts.
  repeated int32 watermark = 1;
}

message GlobalCutOrNoop {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof value {
    GlobalCut global_cut = 1;
    Noop noop = 2;
  }
}


// Protocol messages. //////////////////////////////////////////////////////////
message Phase1a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;

  // The leader knows that all entries in slots less than `chosen_watermark`
  // have been chosen. Acceptors do not have to include slots below
  // `chosen_watermark` in their Phase1b responses.
  //
  // The leader may know that some entries larger than `chosen_watermark` have
  // also been chosen, but that's okay. It's not unsafe for acceptors to return
  // too much information.
  required int32 chosen_watermark = 2;
}

message Phase1bSlotInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 vote_round = 2;
  required GlobalCutOrNoop vote_value = 3;
}

message Phase1b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 acceptor_index = 1;
  required int32 round = 2;
  repeated Phase1bSlotInfo info = 3;
}

message ClientRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required Command command = 1;
}

message Backup {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 server_index = 1;
  required int32 slot = 2;
  required Command command = 3;
}

message ShardInfo {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // Every Scalog server is the primary for a log. This log gets filled in from
  // left to right, without ever having holes. The watermark w is the number of
  // log entries in the log. Equivalently, every log entry less than w has a
  // log entry in it, and every log entry greater than or equal to w doesn't.
  //
  // Every shard has a number of servers. Every server in a shard is the
  // primary for its log and is a backup for the other servers shards. Thus, if
  // a shard has n servers, every server has n watermarks. A ShardInfo includes
  // a servers watermarks. We assume there is an ordering over the servers. The
  // watermarks are in the same order.
  //
  // For example, consider a Scalog deployment with 2 shards, with 2 servers
  // per shard. The ShardCut {shard_index: 1, servers_index: 0, watermark: [1,
  // 3]} represents a shard cut from the first server in second shard with 1
  // entry in the first log and 3 in the second.
  required int32 shard_index = 1;
  required int32 server_index = 2;
  repeated int32 watermark = 3;
}

message ProposeCut {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required GlobalCut global_cut = 1;
}

message Phase2a {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required int32 round = 2;
  required GlobalCutOrNoop global_cut_or_noop = 3;
}

message Phase2b {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 acceptor_index = 1;
  required int32 slot = 2;
  required int32 round = 3;
}

message RawCutChosen {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required GlobalCutOrNoop raw_cut_or_noop = 2;
}

message CutChosen {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
  required GlobalCut cut = 2;
}

message Chosen {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  // A command batch that starts at slot `slot`. For example, if `slot` is 1,
  // and `command_batch` is [x, y, z], then the commands x, y, and z should
  // appear in slots 1, 2, and 3 respectively.
  required int32 slot = 1;
  required CommandBatch command_batch = 2;
}

message ClientReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required CommandId command_id = 1;
  required int32 slot = 2;
  required bytes result = 3;
}

message LeaderInfoRequest {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";
}

message LeaderInfoReply {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

message Recover {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 slot = 1;
}

message Nack {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  required int32 round = 1;
}

// Inbound messages. ///////////////////////////////////////////////////////////
message ClientInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientReply client_reply = 1;
  }
}

message ServerInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ClientRequest client_request = 1;
    Backup backup = 2;
    CutChosen cut_chosen = 3;
    Recover recover = 4;
  }
}

message AggregatorInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    ShardInfo shard_info = 1;
    RawCutChosen raw_cut_chosen = 2;
    LeaderInfoReply leader_info_reply = 3;
    Recover recover = 4;
  }
}

message LeaderInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase1b phase1b = 1;
    ProposeCut propose_cut = 2;
    Phase2b phase2b = 3;
    RawCutChosen raw_cut_chosen = 4;
    LeaderInfoRequest leader_info_request = 5;
    Recover recover = 6;
    Nack nack = 7;
  }
}

message AcceptorInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Phase1a phase1a = 1;
    Phase2a phase2a = 2;
  }
}

message ReplicaInbound {
  option (scalapb.message).annotations =
    "@scala.scalajs.js.annotation.JSExportAll";

  oneof request {
    Chosen chosen = 1;
  }
}
